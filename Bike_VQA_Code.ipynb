{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ppbvbGkrR-7W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IcQwBemnJNPm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "6k8kt3_hHUMb"
      },
      "outputs": [],
      "source": [
        "# WORKING_DIR = '/content/drive/MyDrive/bike'\n",
        "# IMAGE_DIR = '/content/drive/MyDrive/bike/images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oxfPtPl4Hjms"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel(\"qa3.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "dNGg8ya6H--Z",
        "outputId": "c7472a64-6006-4cab-8c11-ea860bdddfa7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>image_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>is the person wearing helmet?</td>\n",
              "      <td>yes</td>\n",
              "      <td>images\\image1.jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is the person wearing helmet?</td>\n",
              "      <td>no</td>\n",
              "      <td>images\\image2.jpeg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        question answer            image_id\n",
              "0  is the person wearing helmet?    yes  images\\image1.jpeg\n",
              "1  is the person wearing helmet?     no  images\\image2.jpeg"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['image_id'] = data['image_id'].apply(lambda img: os.path.join('images', img+'.jpeg'))\n",
        "data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "olMqv5EvJj7o"
      },
      "outputs": [],
      "source": [
        "# WORKING_DIR = '/content/drive/MyDrive/pathvqa'\n",
        "# IMAGE_DIR = '/content/drive/MyDrive/pathvqa/images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "o5nG21f4GnKs"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>image_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>is the person wearing helmet?</td>\n",
              "      <td>yes</td>\n",
              "      <td>images\\image1.jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is the person wearing helmet?</td>\n",
              "      <td>no</td>\n",
              "      <td>images\\image2.jpeg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        question answer            image_id\n",
              "0  is the person wearing helmet?    yes  images\\image1.jpeg\n",
              "1  is the person wearing helmet?     no  images\\image2.jpeg"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "VQGuL-8Nn3n8"
      },
      "outputs": [],
      "source": [
        "data['answer']=data['answer'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "5o9qIVd8oD9x"
      },
      "outputs": [],
      "source": [
        "x=data.drop(columns=['answer'])\n",
        "y=data['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "p_H1icVVoNPf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II_xhamUTqWi",
        "outputId": "18a45055-5aa8-4bd2-e084-762d68661652"
      },
      "outputs": [],
      "source": [
        "# !pip install keras_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "TPaAg0hhSj5t"
      },
      "outputs": [],
      "source": [
        "# from keras_preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOfNOEfmIIc1",
        "outputId": "a98305a1-4ee6-4e00-e2b4-8f910cd570bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29\n"
          ]
        }
      ],
      "source": [
        "max_length = data['question'].str.len().max()\n",
        "print(max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "UiEa5HFAThFh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "CAolCXi_Tx7S"
      },
      "outputs": [],
      "source": [
        "def textokenizer(text):\n",
        "  max_length = 24\n",
        "  tok = Tokenizer()\n",
        "  tok.fit_on_texts(x_train['question'])\n",
        "  vocab_size = len(tok.word_index) + 1\n",
        "  #print('Total unique words in the x_train',vocab_size)\n",
        "  encoded_text = tok.texts_to_sequences(text)\n",
        "  padded_text = pad_sequences(encoded_text, maxlen=max_length)  #padding zeros at the begining of each question so that each sequence will have same length\n",
        "  #print(padded_text.shape)\n",
        "  return padded_text, tok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoZ_K2pmT683",
        "outputId": "6c8bb696-42d2-4cbe-ea00-a99b09552ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "#Each token is represented using 300-dim vector using pre-trained GloVe representation.\n",
        "with open( 'glove_vector.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "    glove_words =  set(model.keys())\n",
        "\n",
        "# for train\n",
        "_,tok = textokenizer(x_train['question'])\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "embedding_matrix_train = np.zeros((vocab_size, 300))\n",
        "for word, i in tok.word_index.items():\n",
        "    if word in glove_words:\n",
        "        embedding_vector = model[word]\n",
        "        embedding_matrix_train[i] = embedding_vector\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "66D-LQHCUCDx"
      },
      "outputs": [],
      "source": [
        "##print(\"embedding matrix shape\",embedding_matrix_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "qEABei2I3uWy"
      },
      "outputs": [],
      "source": [
        "# with open( 'embedding_matrix_train.pkl'), 'wb') as f:\n",
        "#     pickle.dump(embedding_matrix_train, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "fcwCkUUD3yBK"
      },
      "outputs": [],
      "source": [
        "# with open(os.path.join(WORKING_DIR, 'embedding_matrix_train.pkl'), 'rb') as f:\n",
        "#     embedding_matrix_train = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "2sb8VDJdUUmY"
      },
      "outputs": [],
      "source": [
        "def optokens(classes):\n",
        "  from sklearn.preprocessing import OneHotEncoder\n",
        "  ohe=OneHotEncoder(handle_unknown='ignore')\n",
        "  ohe.fit(y_train.values.reshape(-1,1))\n",
        "  optoken=ohe.transform(classes.values.reshape(-1,1)).toarray()\n",
        "  return optoken, ohe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "s4gvNJGRUWyq"
      },
      "outputs": [],
      "source": [
        "_,ohe=optokens(y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "2l9vypDHUaxw"
      },
      "outputs": [],
      "source": [
        "pickle.dump(ohe,open('ohe.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7yEfTOnUmVz",
        "outputId": "aca88dcb-afbe-4ea4-c4e2-a3f7dff586eb"
      },
      "outputs": [],
      "source": [
        "# model = VGG16(weights='imagenet')\n",
        "# feature_model = Model(inputs=model.inputs, outputs=model.layers[-2].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "50wcV4v-XTRC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "PLC3rvruXEav"
      },
      "outputs": [],
      "source": [
        "def parse_function(filename):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    #Don't use tf.image.decode_image, or the output shape will be undefined\n",
        "    image = tf.image.decode_png(image_string, channels=3)\n",
        "    image = image/255\n",
        "    #This will convert to float values in [0, 1]\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image.set_shape((224, 224, 3))\n",
        "    return  image, filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cba2228985184a2db66eb0bd0707f2e9",
            "8b4fea39ae2047ecaaa8cb7a475e98ba",
            "52fd313001094b6bae09d8ecb3e1f9d3",
            "042ea42b97c74490a4e4818eeea197f6",
            "edd375fb16f142d795340363b45139a8",
            "8d1c9026d9634eaf8820d40c5bd1b31e",
            "e8dfca268a2140b2912fdf8fe4bd8e17",
            "12d133a7ea784b02a47ec20005e634d0",
            "742dc88ccebc4d9dabf933302223801c",
            "df1807ef6cf1451ab5c86c9f63acfcee",
            "be45d145d5ae48ec857afa9cbcb8a8b1"
          ]
        },
        "id": "0__v8BUqXRa5",
        "outputId": "2a05ff10-eddc-42ab-cef0-dcc7a8c710af"
      },
      "outputs": [],
      "source": [
        "# img_fl=sorted(set(data['image_id'].tolist()))\n",
        "# img_data_tr=tf.data.Dataset.from_tensor_slices(img_fl)\n",
        "# img_data_tr=img_data_tr.map(parse_function,num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "# for img, path in tqdm(img_data_tr):\n",
        "#   batch_features = feature_model(img)\n",
        "#   #batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "#   for bf, p in zip(batch_features, path):\n",
        "#     path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "#     np.save(path_of_feature, bf.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "JPi0AX3z5FiJ"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "\n",
        "# image_filenames = x_train['image_id']\n",
        "# # Path to your images directory\n",
        "# images_directory = IMAGE_DIR\n",
        "\n",
        "# # Choose the first image ID\n",
        "# first_image_id = image_filenames.iloc[0]\n",
        "\n",
        "# # Loop through the data and find the first occurrence of the first image ID\n",
        "# for image_filename, question, answer in zip(image_filenames, ques_train, answ_train):\n",
        "#     if image_filename == first_image_id:\n",
        "#         # Construct the full path to the image\n",
        "#         image_path = image_filename\n",
        "\n",
        "#         # Load and display the image\n",
        "#         image = Image.open(image_path)\n",
        "#         plt.imshow(image)\n",
        "#         plt.title(\"Image Filename: {}\\nQuestion: {}\\nAnswer: {}\".format(image_filename, question, answer))\n",
        "#         plt.axis('off')\n",
        "#         plt.show()\n",
        "#         break  # Break the loop after displaying the first image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "tkn2qFy7523o"
      },
      "outputs": [],
      "source": [
        "ques_train = x_train['question']\n",
        "answ_train = y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "DfUQQOngc4Xn"
      },
      "outputs": [],
      "source": [
        "ques_train , _ =textokenizer(x_train['question'])\n",
        "answ_train , _ =optokens(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "nk3Imaf7Zgd5"
      },
      "outputs": [],
      "source": [
        "# print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEPnZ3w4Dv9_",
        "outputId": "b1769106-3b37-4b34-adac-9697fcdb8f59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(147, 24)\n",
            "(147, 2)\n"
          ]
        }
      ],
      "source": [
        "print(ques_train.shape)\n",
        "print(answ_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "5Xs6Kmuzc9SI"
      },
      "outputs": [],
      "source": [
        "ques_train=ques_train.tolist()\n",
        "answ_train=answ_train.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "4Wg3Y1Jz3r4g"
      },
      "outputs": [],
      "source": [
        "# print(ques_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "hG7eKDAIcdts"
      },
      "outputs": [],
      "source": [
        "# print(answ_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "_802CWQfdBX2"
      },
      "outputs": [],
      "source": [
        "def map_func(img_name, question):\n",
        "    img_tensor = np.load(str(img_name) + '.npy')  # Ensure img_name is treated as a string\n",
        "    # print(img_tensor)\n",
        "    return img_tensor, question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "7mdwYB222Vy-"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "Ge0aowbq5kW6"
      },
      "outputs": [],
      "source": [
        "image_filenames = x_train['image_id']\n",
        "\n",
        "preprocessed_data = [(map_func(image_filenames, question), answer) for image_filenames, question, answer in zip(image_filenames, ques_train, answ_train)]\n",
        "\n",
        "# Extract image features and tokenized questions from preprocessed data\n",
        "image_features = np.array([item[0][0] for item in preprocessed_data])\n",
        "tokenized_questions = np.array([item[0][1] for item in preprocessed_data])\n",
        "answers = np.array([item[1] for item in preprocessed_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "_KmgmZ3QOeX3"
      },
      "outputs": [],
      "source": [
        "#image_features=np.resize(image_features,(224,224,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "bckp7QmHzC_-"
      },
      "outputs": [],
      "source": [
        "# print(preprocessed_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc5FP8W5raOX",
        "outputId": "50e1a15d-496f-4697-d677-919d6327dba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "147\n",
            "147\n",
            "147\n"
          ]
        }
      ],
      "source": [
        "print(len(image_features))\n",
        "print(len(tokenized_questions))\n",
        "print(len(answers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "0kCpRsUHrqsJ"
      },
      "outputs": [],
      "source": [
        "# print(image_features[0])\n",
        "# print(tokenized_questions[0])\n",
        "# print(answers[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "x_L9uqTL3aeb"
      },
      "outputs": [],
      "source": [
        "# print(tokenized_questions[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkn4RJAVoPOd",
        "outputId": "6be360c0-43de-44f1-bf56-d007bfe882fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "147\n",
            "<_PrefetchDataset element_spec=((TensorSpec(shape=(None, 4096), dtype=tf.float32, name=None), TensorSpec(shape=(None, 24), dtype=tf.int64, name=None)), TensorSpec(shape=(None, 2), dtype=tf.float64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "tokenized_questions = tf.cast(tokenized_questions, tf.int64)\n",
        "x_dataset = tf.data.Dataset.from_tensor_slices((image_features, tokenized_questions))\n",
        "# answers = tf.cast(answers, tf.int64)\n",
        "# Combine answers into a separate dataset\n",
        "y_dataset = tf.data.Dataset.from_tensor_slices(answers)\n",
        "\n",
        "# Combine x and y datasets into a single dataset\n",
        "train_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))\n",
        "\n",
        "print(len(train_dataset))\n",
        "\n",
        "# Batch and prefetch the dataset\n",
        "batch_size = 12\n",
        "train_dataset = train_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Print dataset information\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "1l4t_BmNzE1V"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# # Convert PrefetchDataset to an iterable\n",
        "# iter_train_dataset = iter(train_dataset)\n",
        "\n",
        "# # Get the first element\n",
        "# first_element = next(iter_train_dataset)\n",
        "# second_element = next(iter_train_dataset)\n",
        "\n",
        "\n",
        "# # Print the first element\n",
        "# print(first_element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI6wIbuotfFc",
        "outputId": "8ca26680-7c42-41e4-fd82-dc23412c8486"
      },
      "outputs": [],
      "source": [
        "tf.data.experimental.save(train_dataset,  'train_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "FrB6-Ha6uE90"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.experimental.load('train_dataset', element_spec = ((tf.TensorSpec(shape=(None, 4096), dtype=tf.float32),\n",
        "                 tf.TensorSpec(shape=(None, 24), dtype=tf.int64)),\n",
        "                tf.TensorSpec(shape=(None, 2), dtype=tf.float64)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "ZCefEpkrdKJY"
      },
      "outputs": [],
      "source": [
        "ques_test,_=textokenizer(x_test['question'])\n",
        "answ_test,_=optokens(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "iHU0ebWFwHLV"
      },
      "outputs": [],
      "source": [
        "# print(ques_test[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "8FQjhHAldSw0"
      },
      "outputs": [],
      "source": [
        "ques_test=ques_test.tolist()\n",
        "answ_test=answ_test.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "XYG3W8mwovos"
      },
      "outputs": [],
      "source": [
        "image_filenames = x_test['image_id']\n",
        "\n",
        "# print(image_filenames[1])\n",
        "# Preprocess images and questions using map_func\n",
        "preprocessed_data = [(map_func(image_filenames, question), answer) for image_filenames, question, answer in zip(image_filenames, ques_test, answ_test)]\n",
        "\n",
        "# Extract image features and tokenized questions from preprocessed data\n",
        "image_features = np.array([item[0][0] for item in preprocessed_data])\n",
        "tokenized_questions = np.array([item[0][1] for item in preprocessed_data])\n",
        "answers = np.array([item[1] for item in preprocessed_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjEubqfc08q6",
        "outputId": "20acf6aa-ce87-413f-ea90-a2b9e1088df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63\n",
            "63\n",
            "63\n"
          ]
        }
      ],
      "source": [
        "print(len(image_features))\n",
        "print(len(tokenized_questions))\n",
        "print(len(answers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9iF4t7tpTM9",
        "outputId": "d1c9715f-32ed-41bd-ea48-61fc017260bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_PrefetchDataset element_spec=((TensorSpec(shape=(None, 4096), dtype=tf.float32, name=None), TensorSpec(shape=(None, 24), dtype=tf.int64, name=None)), TensorSpec(shape=(None, 2), dtype=tf.float64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "tokenized_questions = tf.cast(tokenized_questions, tf.int64)\n",
        "xo_dataset = tf.data.Dataset.from_tensor_slices((image_features, tokenized_questions))\n",
        "\n",
        "# answers = tf.cast(answers, tf.int64)\n",
        "# Combine answers into a separate dataset\n",
        "yo_dataset = tf.data.Dataset.from_tensor_slices(answers)\n",
        "\n",
        "# Combine x and y datasets into a single dataset\n",
        "test_dataset = tf.data.Dataset.zip((xo_dataset, yo_dataset))\n",
        "\n",
        "# Batch and prefetch the dataset\n",
        "batch_size = 12\n",
        "test_dataset = test_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Print dataset information\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "B2l5K8z8uJkV"
      },
      "outputs": [],
      "source": [
        "tf.data.experimental.save(test_dataset, 'test_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "P0P4bLmfuL2E"
      },
      "outputs": [],
      "source": [
        "test_dataset = tf.data.experimental.load( 'test_dataset', element_spec = ((tf.TensorSpec(shape=(None, 4096), dtype=tf.float32),\n",
        "                 tf.TensorSpec(shape=(None, 24), dtype=tf.int64)),\n",
        "                tf.TensorSpec(shape=(None, 2), dtype=tf.float64)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "CUYIhl6RgBQR"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras import initializers\n",
        "# from tensorflow.keras.regularizers import l2\n",
        "# from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Flatten, BatchNormalization, Dropout, multiply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "hV8rVr_PdaSD"
      },
      "outputs": [],
      "source": [
        "#IMAGE MODEL\n",
        "#im_input = Input(shape=(4096,), name = \"im_input\")\n",
        "#flat = Flatten()(im_input)\n",
        "#image_model1=Dense(50,kernel_initializer=initializers.he_normal(seed=42))(flat)\n",
        "#image_model=Model(inputs=im_input,outputs=image_model1)\n",
        "#image_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-Kj9SUlvMbtN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "iQosioAZMs5E"
      },
      "outputs": [],
      "source": [
        "# from keras.layers import Conv1D,MaxPool1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "4LK_UvVoJdKI"
      },
      "outputs": [],
      "source": [
        "# #IMAGE MODEL\n",
        "# im_input = Input(shape=(4096,1), name = \"im_input\")\n",
        "# #flat = Flatten()(im_input)\n",
        "# layer1= (Conv1D(input_shape=(4096,1),filters=64,kernel_size=(3),padding=\"same\", activation=\"relu\"))(im_input)\n",
        "# layer2=(Conv1D(filters=64,kernel_size=(3),padding=\"same\", activation=\"relu\"))(layer1)\n",
        "# layer3=(MaxPool1D(pool_size=(2),strides=(2)))(layer2)\n",
        "# d1 = Dropout(0.2)(layer3)\n",
        "# layer4=(Conv1D(filters=128, kernel_size=(3), padding=\"same\", activation=\"relu\"))(d1)\n",
        "# layer5=(Conv1D(filters=128, kernel_size=(3), padding=\"same\", activation=\"relu\"))(layer4)\n",
        "# layer6=(MaxPool1D(pool_size=(2),strides=(2))(layer5))\n",
        "# layer7=(Conv1D(filters=256, kernel_size=(3), padding=\"same\", activation=\"relu\")(layer6))\n",
        "# layer8=(MaxPool1D(pool_size=(2),strides=(2)))(layer7)\n",
        "# flat = Flatten()(layer8)\n",
        "# image_model1=Dense(50,kernel_initializer=initializers.he_normal(seed=42))(flat)\n",
        "# image_model=Model(inputs=im_input,outputs=image_model1)\n",
        "# image_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "xhw5r4Fzdeg1"
      },
      "outputs": [],
      "source": [
        "# max_length=24\n",
        "# vocab_size=embedding_matrix_train.shape[0]\n",
        "# print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "e36SF5KqdjIW"
      },
      "outputs": [],
      "source": [
        "# #QUESTION MODEL\n",
        "# from tensorflow.keras.layers import LSTM\n",
        "# ques_input = Input(shape=(max_length,), name = \"ques_input\")\n",
        "# e1 = Embedding(vocab_size, 300, weights=[embedding_matrix_train], input_length=131, trainable=False)(ques_input)\n",
        "# lstm1= LSTM(64,kernel_initializer=initializers.he_normal(seed=42),kernel_regularizer=l2(0.001),return_sequences=True)(e1)\n",
        "# lstm2= LSTM(64,kernel_initializer=initializers.he_normal(seed=42),kernel_regularizer=l2(0.001),return_sequences=True)(lstm1)\n",
        "# #l1= LeakyReLU(alpha = 0.3)(l1)\n",
        "# f1= Flatten(name='flatten_2')(lstm2)\n",
        "# question_model1=Dense(50,kernel_initializer=initializers.he_normal(seed=42))(f1)\n",
        "# question_model = Model(inputs=ques_input, outputs=question_model1)\n",
        "# question_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "TOawTPt-RqQ6"
      },
      "outputs": [],
      "source": [
        "# print(image_model.layers[-1].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "4FhWMa_3dlNj"
      },
      "outputs": [],
      "source": [
        "# #COMBINING FEATURES AND MAKING FINAL MODEL FOR PREDICTION\n",
        "# from tensorflow.keras.layers import multiply\n",
        "# from keras import regularizers\n",
        "# input_model=multiply([image_model.layers[-1].output,question_model.layers[-1].output])\n",
        "# d1=BatchNormalization()(input_model)\n",
        "# d1 = Dropout(0.2)(d1)\n",
        "# d1=Dense(50,kernel_initializer=initializers.he_normal(seed=42),kernel_regularizer=regularizers.l2(0.01))(d1)\n",
        "# d1 = Dropout(0.2)(d1)\n",
        "# final_output = Dense(8, kernel_initializer=initializers.he_normal(seed=42),kernel_regularizer=regularizers.l2(0.01),activation='softmax')(d1)\n",
        "# final_model = Model(inputs=[im_input,ques_input], outputs=final_output)\n",
        "# print(final_model.summary())\n",
        "# plot_model(final_model, show_shapes=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "qo16UU0FdnYv"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "# !rm -rf ./logs1/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "Tcstx1Odds0b"
      },
      "outputs": [],
      "source": [
        "# checkpoint = ModelCheckpoint(\"basic_model1.h5\",\n",
        "#                              monitor=\"val_loss\",\n",
        "#                              mode=\"min\",\n",
        "#                              save_best_only = True,\n",
        "#                              verbose=1)\n",
        "# # earlystop= EarlyStopping(monitor = 'val_loss',\n",
        "# #                             mode=\"min\",\n",
        "# #                             min_delta = 0,\n",
        "# #                             patience = 5,\n",
        "# #                             verbose = 1)\n",
        "\n",
        "# tensorboard = TensorBoard(log_dir='logs1',histogram_freq=1,write_grads=True)\n",
        "\n",
        "# callbacks = [checkpoint,tensorboard]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xZ1_orPIhyCQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "IOCczXZMbEga",
        "outputId": "a3241841-b659-4859-e297-5a345ecca0fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model('model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt_QWEsHdy6y",
        "outputId": "dbb4e186-be55-4c33-cb29-3baf224ca9a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 289ms/step - accuracy: 0.3851 - loss: 0.7408 - val_accuracy: 0.3016 - val_loss: 0.7158\n",
            "Epoch 2/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 208ms/step - accuracy: 0.4671 - loss: 0.6995 - val_accuracy: 0.3333 - val_loss: 0.6988\n",
            "Epoch 3/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 207ms/step - accuracy: 0.5328 - loss: 0.6919 - val_accuracy: 0.3492 - val_loss: 0.6914\n",
            "Epoch 4/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 217ms/step - accuracy: 0.5621 - loss: 0.6861 - val_accuracy: 0.5238 - val_loss: 0.6835\n",
            "Epoch 5/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - accuracy: 0.5955 - loss: 0.6749 - val_accuracy: 0.6667 - val_loss: 0.6687\n",
            "Epoch 6/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 214ms/step - accuracy: 0.6180 - loss: 0.6589 - val_accuracy: 0.6667 - val_loss: 0.6451\n",
            "Epoch 7/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 210ms/step - accuracy: 0.6723 - loss: 0.6061 - val_accuracy: 0.6667 - val_loss: 0.6175\n",
            "Epoch 8/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 210ms/step - accuracy: 0.6731 - loss: 0.5597 - val_accuracy: 0.6825 - val_loss: 0.6471\n",
            "Epoch 9/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step - accuracy: 0.7194 - loss: 0.5141 - val_accuracy: 0.6984 - val_loss: 0.6712\n",
            "Epoch 10/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 211ms/step - accuracy: 0.8266 - loss: 0.4344 - val_accuracy: 0.6825 - val_loss: 0.7842\n",
            "Epoch 11/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 210ms/step - accuracy: 0.9092 - loss: 0.3429 - val_accuracy: 0.6825 - val_loss: 0.8826\n",
            "Epoch 12/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 214ms/step - accuracy: 0.9364 - loss: 0.2665 - val_accuracy: 0.6825 - val_loss: 0.9318\n",
            "Epoch 13/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 215ms/step - accuracy: 0.8940 - loss: 0.2521 - val_accuracy: 0.6825 - val_loss: 0.9052\n",
            "Epoch 14/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 212ms/step - accuracy: 0.9770 - loss: 0.1740 - val_accuracy: 0.6984 - val_loss: 0.6838\n",
            "Epoch 15/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 217ms/step - accuracy: 0.9689 - loss: 0.1420 - val_accuracy: 0.7460 - val_loss: 0.5612\n",
            "Epoch 16/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 213ms/step - accuracy: 0.9790 - loss: 0.1276 - val_accuracy: 0.7460 - val_loss: 0.6129\n",
            "Epoch 17/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 208ms/step - accuracy: 0.9840 - loss: 0.1144 - val_accuracy: 0.7302 - val_loss: 0.8023\n",
            "Epoch 18/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 206ms/step - accuracy: 0.9810 - loss: 0.0982 - val_accuracy: 0.7460 - val_loss: 0.8240\n",
            "Epoch 19/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 213ms/step - accuracy: 0.9869 - loss: 0.0859 - val_accuracy: 0.7778 - val_loss: 0.5114\n",
            "Epoch 20/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 209ms/step - accuracy: 0.9818 - loss: 0.0679 - val_accuracy: 0.7937 - val_loss: 0.3720\n",
            "Epoch 21/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 204ms/step - accuracy: 0.9965 - loss: 0.0590 - val_accuracy: 0.8571 - val_loss: 0.3218\n",
            "Epoch 22/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 207ms/step - accuracy: 0.9719 - loss: 0.0758 - val_accuracy: 0.9524 - val_loss: 0.1315\n",
            "Epoch 23/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 208ms/step - accuracy: 0.9895 - loss: 0.0412 - val_accuracy: 0.9841 - val_loss: 0.0751\n",
            "Epoch 24/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 205ms/step - accuracy: 0.9865 - loss: 0.0513 - val_accuracy: 0.9524 - val_loss: 0.1084\n",
            "Epoch 25/25\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 207ms/step - accuracy: 0.9776 - loss: 0.0521 - val_accuracy: 0.9206 - val_loss: 0.2322\n"
          ]
        }
      ],
      "source": [
        "# # Compile your final_model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Compile the model\n",
        "# final_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# generator = data_generator(train_dataset)\n",
        "# final_model.fit(generator, epochs= 15, steps_per_epoch=len(train_dataset), verbose = 1, callbacks = callbacks, validation_data = test_dataset)\n",
        "\n",
        "\n",
        "# Train your model\n",
        "h1 = model.fit(train_dataset, epochs=25, verbose=1, validation_data = test_dataset)\n",
        "\n",
        "#h1 = final_model.fit(train_dataset, epochs=100,\n",
        "#                      validation_data=test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjO8VhqhNH0f",
        "outputId": "4d37d7a3-47fe-4043-ca7b-161a7d508386"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save( 'model_bike.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "x1x_xb-pavmo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model('model_bike.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGwiOZQWydov",
        "outputId": "e84e83d9-9828-4159-e03d-bac8f9af99fe"
      },
      "outputs": [],
      "source": [
        "# for i in range (len(x_test)):\n",
        "#   tok = Tokenizer()\n",
        "#   import numpy as np\n",
        "#   # Evaluate the model on a single row\n",
        "#   row = x_train.iloc[i]\n",
        "#   image_features = np.load(str(row['image_id']) + '.npy')\n",
        "#   print(\"image_features: \", image_features)\n",
        "#   print(\"image id:\", row['image_id'])\n",
        "#   print(\"question:\", row[['question']])\n",
        "#   tokenized_question = tok.texts_to_sequences([row['question']])\n",
        "#   tokenized_question = pad_sequences(tokenized_question, maxlen=24)\n",
        "#   prediction = model.predict([image_features.reshape(1, -1), tokenized_question])\n",
        "#   print(\"predicted answer:\", ohe.inverse_transform(prediction))\n",
        "#   print(\"predicted answer:\", prediction)\n",
        "#   print(\"actual answer:\", y_test.iloc[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiRoBre-IIG5",
        "outputId": "27e0f4f8-7e3c-4425-9b48-1f804ef5a104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "predicted answer: [['no']]\n",
            "actual answer: no\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "predicted answer: [['yes']]\n",
            "actual answer: yes\n"
          ]
        }
      ],
      "source": [
        "for i in range (len(x_test)):\n",
        "  tok = Tokenizer()\n",
        "  import numpy as np\n",
        "  # Evaluate the model on a single row\n",
        "  row = data.iloc[i]\n",
        "  image_features = np.load(str(row['image_id']) + '.npy')\n",
        "  tokenized_question = tok.texts_to_sequences([row['question']])\n",
        "  tokenized_question = pad_sequences(tokenized_question, maxlen=24)\n",
        "  prediction = model.predict([image_features.reshape(1, -1), tokenized_question])\n",
        "  print(\"predicted answer:\", ohe.inverse_transform(prediction))\n",
        "  print(\"actual answer:\", data['answer'][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "Img_model = VGG16(weights='imagenet')\n",
        "feature_model = Model(inputs=Img_model.inputs, outputs=Img_model.layers[-2].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the pickle file\n",
        "with open('ohe.pkl', 'rb') as file:\n",
        "    ohe = pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "[['yes']]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the trained model\n",
        "# model = tf.keras.models.load_model('model.h5')\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['question'])\n",
        "\n",
        "def parse_function(filename):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    #Don't use tf.image.decode_image, or the output shape will be undefined\n",
        "    image = tf.image.decode_png(image_string, channels=3)\n",
        "    image = image/255\n",
        "    #This will convert to float values in [0, 1]\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = tf.expand_dims(image, axis=0) \n",
        "    return  image, filename\n",
        "\n",
        "def predict_answer(image_path, question):\n",
        "    # Load and preprocess the image\n",
        "    \n",
        "    # Tokenize and pad the question\n",
        "    tokenized_question = tokenizer.texts_to_sequences([question])\n",
        "    tokenized_question = pad_sequences(tokenized_question, maxlen=24)\n",
        "\n",
        "    # Reshape the image and question for model input\n",
        "    image,_ = parse_function(image_path)\n",
        "    image_features = feature_model(image)\n",
        "    tokenized_question = tokenized_question.reshape(1, -1)\n",
        "\n",
        "    # Make the prediction\n",
        "    prediction = model.predict([image_features, tokenized_question])\n",
        "\n",
        "    # Decode the predicted answer\n",
        "    predicted_answer = ohe.inverse_transform(prediction)\n",
        "\n",
        "    return predicted_answer\n",
        "\n",
        "# Example usage\n",
        "image_path = 'testImg4.jpeg'\n",
        "question = 'is the person wearing helmet?'\n",
        "predicted_answer = predict_answer(image_path, question)\n",
        "print(predicted_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/05/01 10:12:23 [W] [service.go:132] login to server failed: dial tcp 44.237.78.176:7000: i/o timeout\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the trained model\n",
        "# model = tf.keras.models.load_model('model.h5')\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['question'])\n",
        "\n",
        "def parse_function(filename):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    # Don't use tf.image.decode_image, or the output shape will be undefined\n",
        "    image = tf.image.decode_png(image_string, channels=3)\n",
        "    image = image/255\n",
        "    # This will convert to float values in [0, 1]\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = tf.expand_dims(image, axis=0) \n",
        "    return  image, filename\n",
        "\n",
        "# def parse_function(image_data):\n",
        "#     # Decode the image data using TensorFlow\n",
        "#     image = tf.image.decode_image(image_data, channels=3)\n",
        "#     # Resize the image\n",
        "#     image = tf.image.resize(image, [224, 224])\n",
        "#     # Normalize the pixel values to [0, 1]\n",
        "#     image = image / 255.0\n",
        "#     # Add a batch dimension\n",
        "#     image = tf.expand_dims(image, axis=0)\n",
        "#     return image, None  # We don't need the file name for prediction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_answer(image, question):\n",
        "    # Load and preprocess the image\n",
        "    \n",
        "    # Tokenize and pad the question\n",
        "    tokenized_question = tokenizer.texts_to_sequences([question])\n",
        "    tokenized_question = pad_sequences(tokenized_question, maxlen=24)\n",
        "\n",
        "    # Reshape the image and question for model input\n",
        "    image_features, _ = parse_function(image)\n",
        "    image_features = feature_model(image_features)  # If you have a feature extraction model\n",
        "    tokenized_question = tokenized_question.reshape(1, -1)\n",
        "\n",
        "    # Make the prediction\n",
        "    # prediction = model.predict([image_features, tokenized_question])\n",
        "\n",
        "    # Decode the predicted answer\n",
        "    # You will need to replace this with your actual prediction logic\n",
        "    predicted_answer = model.predict([image_features, tokenized_question])\n",
        "    predicted_answer = ohe.inverse_transform(predicted_answer)\n",
        "    predicted_answer = predicted_answer[0][0]\n",
        "    return predicted_answer\n",
        "\n",
        "# Gradio Interface\n",
        "image_input = gr.Image(type='filepath', label=\"Upload Image\")\n",
        "question_input = gr.Textbox(label=\"Enter your question here:\", placeholder=\"e.g., Is the person wearing a helmet?\")\n",
        "output = gr.Textbox(label=\"Prediction\")\n",
        "\n",
        "gr.Interface(fn=predict_answer, inputs=[image_input, question_input], outputs=output, title=\"VQA Model\", description=\"Predict whether a person is wearing a helmet based on an image and a question.\").launch(share = True)\n",
        "# ans = predict_answer('testImg4.jpeg', 'is the person wearing helmet?')\n",
        "# print(ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "042ea42b97c74490a4e4818eeea197f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df1807ef6cf1451ab5c86c9f63acfcee",
            "placeholder": "​",
            "style": "IPY_MODEL_be45d145d5ae48ec857afa9cbcb8a8b1",
            "value": " 7/7 [03:29&lt;00:00, 26.01s/it]"
          }
        },
        "12d133a7ea784b02a47ec20005e634d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52fd313001094b6bae09d8ecb3e1f9d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12d133a7ea784b02a47ec20005e634d0",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_742dc88ccebc4d9dabf933302223801c",
            "value": 7
          }
        },
        "742dc88ccebc4d9dabf933302223801c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b4fea39ae2047ecaaa8cb7a475e98ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d1c9026d9634eaf8820d40c5bd1b31e",
            "placeholder": "​",
            "style": "IPY_MODEL_e8dfca268a2140b2912fdf8fe4bd8e17",
            "value": "100%"
          }
        },
        "8d1c9026d9634eaf8820d40c5bd1b31e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be45d145d5ae48ec857afa9cbcb8a8b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cba2228985184a2db66eb0bd0707f2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b4fea39ae2047ecaaa8cb7a475e98ba",
              "IPY_MODEL_52fd313001094b6bae09d8ecb3e1f9d3",
              "IPY_MODEL_042ea42b97c74490a4e4818eeea197f6"
            ],
            "layout": "IPY_MODEL_edd375fb16f142d795340363b45139a8"
          }
        },
        "df1807ef6cf1451ab5c86c9f63acfcee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8dfca268a2140b2912fdf8fe4bd8e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edd375fb16f142d795340363b45139a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
